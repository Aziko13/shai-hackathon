{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app.agent as agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf73afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = agent.build_agent_with_router()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = \"Удали таблицу fact_sales из базы данных\"\n",
    "\n",
    "msg = {\"messages\": [{\"role\": \"user\", \"content\": user_text}]}\n",
    "config = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "print(config)\n",
    "response = graph.invoke(msg, config)\n",
    "last_msg = response[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = \"Покажи минимальную и максимальную дату продаж\"\n",
    "\n",
    "msg = {\"messages\": [{\"role\": \"user\", \"content\": user_text}]}\n",
    "config = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "print(config)\n",
    "response = graph.invoke(msg, config)\n",
    "last_msg = response[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc33e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[m for in response[\"messages\"] if m.role == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597239e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import importlib\n",
    "import sys\n",
    "from httpx import request\n",
    "import pytest\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langsmith import testing as t\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.types import Command\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "\n",
    "sys.path.append(\"/Users/aziz/Documents/repos/shai-hackathon\")\n",
    "import tests.evaluation_obs_sql\n",
    "import app.agent as agent\n",
    "import app.prompts as prompts\n",
    "\n",
    "\n",
    "graph = agent.build_agent_with_router()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f73fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_CRITERIA_SYSTEM_PROMPT = \"\"\"\n",
    "You are an evaluator of SQL generation for a natural language to SQL agent.\n",
    "\n",
    "I will give you:\n",
    "1. The user request.\n",
    "2. A list of calls made by the agent to tools.\n",
    "3. The golden (reference) SQL query.\n",
    "\n",
    "Your task:\n",
    "- Evaluate two metrics:\n",
    "  1. **Exec accuracy**: Does the candidate query produce the same result as the golden query, even if the syntax or formatting differs? (Yes/No)\n",
    "  2. **Exact match**: [Ignore aliases] Is the candidate query logically similar to the golden query. Ignore whitespace and capitalization differences? Ignore differences in column aliases? (Yes/No)\n",
    "\n",
    "Return STRICT JSON in this format:\n",
    "{\n",
    "  \"exec_accuracy\": \"<Yes/No>\",\n",
    "  \"exact_match\": \"<Yes/No>\",\n",
    "  \"explanation\": \"<short explanation why>\"\n",
    "}\n",
    "\n",
    "Example:\n",
    "\n",
    "User request: \"How many stores are in the database?\"\n",
    "List of calls: \"SELECT COUNT(*) AS total_stores FROM dict_store;\"\n",
    "Golden SQL: \"SELECT COUNT(*) FROM dict_store;\"\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"exec_accuracy\": \"Yes\",\n",
    "  \"exact_match\": \"No\",\n",
    "  \"explanation\": \"The candidate query executes correctly and returns the same result, but the alias 'AS total_stores' is missing.\"\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa1b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests.evaluation_obs_sql as e_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa92c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []\n",
    "for req, criteria in zip(e_sql.sql_reqsuests, e_sql.sql_answers):\n",
    "    test_cases.append((req, criteria))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30736ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_model = os.getenv(\"EVALUATOR_MODEL\")\n",
    "evaluator_api_base = os.getenv(\"EVALUATOR_API_BASE\")\n",
    "evaluator_api_key = os.getenv(\"EVALUATOR_API_KEY\")\n",
    "\n",
    "\n",
    "class SQLEvaluation(BaseModel):\n",
    "    \"\"\"Evaluation of candidate SQL against golden SQL across two criteria: execution accuracy and exact match.\"\"\"\n",
    "\n",
    "    exec_accuracy: bool = Field(\n",
    "        description=\"Does the candidate SQL produce the same result as the golden SQL (ignoring syntax/formatting differences)?\"\n",
    "    )\n",
    "    exec_accuracy_justification: str = Field(\n",
    "        description=\"Explain why the candidate SQL does or does not produce the same result.\"\n",
    "    )\n",
    "\n",
    "    exact_match: bool = Field(\n",
    "        description=\"Is the candidate SQL textually identical to the golden SQL (ignoring whitespace and capitalization)?\"\n",
    "    )\n",
    "    exact_match_justification: str = Field(\n",
    "        description=\"Explain why the candidate SQL matches or differs from the golden SQL text.\"\n",
    "    )\n",
    "\n",
    "\n",
    "criteria_eval_llm = init_chat_model(\n",
    "    evaluator_model,\n",
    "    openai_api_base=evaluator_api_base,\n",
    "    openai_api_key=evaluator_api_key,\n",
    ")\n",
    "criteria_eval_structured_llm = criteria_eval_llm.with_structured_output(SQLEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65401658",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases[0][0]\n",
    "\n",
    "def format_messages_string(messages: List[Any]) -> str:\n",
    "    \"\"\"Format messages into a single string for analysis.\"\"\"\n",
    "    return \"\\n\".join(message.pretty_repr() for message in messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8a0bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - TOOL CALL: list_tables()\n",
      " - TOOL CALL: describe_table(dict_store)\n",
      " - TOOL CALL: execute_query(SELECT COUNT(*) as total_stores FROM dict_store)\n"
     ]
    }
   ],
   "source": [
    "req = test_cases[0][0]\n",
    "\n",
    "msg = {\"messages\": [{\"role\": \"user\", \"content\": req}]}\n",
    "config = {\"configurable\": {\"thread_id\": str(req)}}\n",
    "\n",
    "result = graph.invoke(msg, config)\n",
    "all_messages_str = format_messages_string(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32502f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_sql = test_cases[0][1]\n",
    "\n",
    "eval_result = criteria_eval_structured_llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\\n\\n Request: {req}\n",
    "                            Tool calls: \\n\\n {all_messages_str} \n",
    "                            \\n\\n Golden SQL: {golden_sql} \\n\\n \n",
    "                Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\",\n",
    "            },\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8ac7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.exec_accuracy, eval_result.exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e76fac6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The candidate SQL uses a column alias 'AS total_stores', whereas the golden SQL does not. Ignoring whitespace and capitalization, the queries are not identical due to this alias.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.exact_match_justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2432c104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The only difference is the use of 'as total_stores' alias in the candidate, which is not present in the golden SQL. Ignoring capitalization and whitespace, the addition of the alias makes the queries logically equivalent but not an exact textual (structure) match as required by the criteria.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.exact_match_justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2d303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24312637",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []\n",
    "for req, criteria in zip(e.reqsuests, e.criterias):\n",
    "    test_cases.append((req, criteria))\n",
    "print(f\"Created {len(test_cases)} test cases\")\n",
    "return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba86f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3c8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
