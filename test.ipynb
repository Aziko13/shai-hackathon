{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app.agent as agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf73afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = agent.build_agent_with_router()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = \"Удали таблицу fact_sales из базы данных\"\n",
    "\n",
    "msg = {\"messages\": [{\"role\": \"user\", \"content\": user_text}]}\n",
    "config = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "print(config)\n",
    "response = graph.invoke(msg, config)\n",
    "last_msg = response[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = \"Покажи минимальную и максимальную дату продаж\"\n",
    "\n",
    "msg = {\"messages\": [{\"role\": \"user\", \"content\": user_text}]}\n",
    "config = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "print(config)\n",
    "response = graph.invoke(msg, config)\n",
    "last_msg = response[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc33e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[m for in response[\"messages\"] if m.role == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597239e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accf2d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aziz/Documents/repos/shai-hackathon/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import importlib\n",
    "import sys\n",
    "from httpx import request\n",
    "import pytest\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langsmith import testing as t\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.types import Command\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "\n",
    "sys.path.append(\"/Users/aziz/Documents/repos/shai-hackathon\")\n",
    "import tests.evaluation_obs_sql\n",
    "import app.agent as agent\n",
    "import app.prompts as prompts\n",
    "\n",
    "\n",
    "graph = agent.build_agent_with_router()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f73fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_CRITERIA_SYSTEM_PROMPT = \"\"\"\n",
    "You are an evaluator of SQL generation for a natural language to SQL agent.\n",
    "\n",
    "I will give you:\n",
    "1. The user request.\n",
    "2. A list of calls made by the agent to tools (the candidate SQL).\n",
    "3. The golden (reference) SQL query.\n",
    "\n",
    "Your task:\n",
    "- Evaluate two metrics:\n",
    "  1. **Exec accuracy**: Does the candidate query produce the same result as the golden query, even if the syntax or formatting differs? (Yes/No)\n",
    "  2. **Exact match**: Is the candidate query logically equivalent to the golden query? \n",
    "     - Ignore whitespace and capitalization.\n",
    "     - Ignore differences in column aliases (e.g. `AS col_name`).\n",
    "     - Ignore ordering of selected columns if the semantics are the same.\n",
    "     - Focus only on whether the same tables, filters, joins, and aggregations are applied.\n",
    "\n",
    "Return STRICT JSON in this format:\n",
    "{\n",
    "  \"exec_accuracy\": \"<Yes/No>\",\n",
    "  \"exact_match\": \"<Yes/No>\",\n",
    "  \"explanation\": \"<short explanation why>\"\n",
    "}\n",
    "\n",
    "Example:\n",
    "\n",
    "User request: \"How many stores are in the database?\"\n",
    "List of calls: \"SELECT COUNT(*) AS total_stores FROM dict_store;\"\n",
    "Golden SQL: \"SELECT COUNT(*) FROM dict_store;\"\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"exec_accuracy\": \"Yes\",\n",
    "  \"exact_match\": \"Yes\",\n",
    "  \"explanation\": \"Both queries count the total number of stores from the same table. The only difference is the alias, which should be ignored.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa1b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests.evaluation_obs_sql as e_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa92c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []\n",
    "for req, criteria in zip(e_sql.sql_reqsuests, e_sql.sql_answers):\n",
    "    test_cases.append((req, criteria))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30736ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_model = os.getenv(\"EVALUATOR_MODEL\")\n",
    "evaluator_api_base = os.getenv(\"EVALUATOR_API_BASE\")\n",
    "evaluator_api_key = os.getenv(\"EVALUATOR_API_KEY\")\n",
    "\n",
    "\n",
    "class SQLEvaluation(BaseModel):\n",
    "    \"\"\"Evaluation of candidate SQL against golden SQL across two criteria: execution accuracy and exact match.\"\"\"\n",
    "\n",
    "    exec_accuracy: bool = Field(\n",
    "        description=\"Does the candidate SQL produce the same result as the golden SQL (ignoring syntax/formatting differences)?\"\n",
    "    )\n",
    "    exec_accuracy_justification: str = Field(\n",
    "        description=\"Explain why the candidate SQL does or does not produce the same result.\"\n",
    "    )\n",
    "\n",
    "    exact_match: bool = Field(\n",
    "        description=\"Is the candidate SQL textually identical to the golden SQL (ignoring whitespace and capitalization)?\"\n",
    "    )\n",
    "    exact_match_justification: str = Field(\n",
    "        description=\"Explain why the candidate SQL matches or differs from the golden SQL text.\"\n",
    "    )\n",
    "\n",
    "\n",
    "criteria_eval_llm = init_chat_model(\n",
    "    evaluator_model,\n",
    "    openai_api_base=evaluator_api_base,\n",
    "    openai_api_key=evaluator_api_key,\n",
    ")\n",
    "criteria_eval_structured_llm = criteria_eval_llm.with_structured_output(SQLEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65401658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages_string(messages: List[Any]) -> str:\n",
    "    \"\"\"Format messages into a single string for analysis.\"\"\"\n",
    "    return \"\\n\".join(message.pretty_repr() for message in messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8a0bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - TOOL CALL: list_tables()\n",
      " - TOOL CALL: describe_table(dict_store)\n",
      " - TOOL CALL: describe_table(fact_sales)\n",
      " - TOOL CALL: execute_query(SELECT ds.store_name, SUM(fs.sales_tg) as total_revenue FROM fact_sales fs JOIN dict_store ds ON fs.store_id = ds.store_id GROUP BY ds.store_name ORDER BY total_revenue DESC)\n",
      " - TOOL CALL: list_tables()\n",
      " - TOOL CALL: describe_table(fact_cost)\n",
      " - TOOL CALL: execute_query(SELECT store_id, sku_id, cost FROM fact_cost)\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for test_case in test_cases[8:]:\n",
    "    req = test_case[0]\n",
    "    golden_sql = test_case[1]\n",
    "\n",
    "    msg = {\"messages\": [{\"role\": \"user\", \"content\": req}]}\n",
    "    config = {\"configurable\": {\"thread_id\": str(req)}}\n",
    "\n",
    "    result = graph.invoke(msg, config)\n",
    "    all_messages_str = format_messages_string(result[\"messages\"])\n",
    "\n",
    "    eval_result = criteria_eval_structured_llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\\n\\n Request: {req}\n",
    "                            Tool calls: \\n\\n {all_messages_str} \n",
    "                            \\n\\n Golden SQL: {golden_sql} \\n\\n \n",
    "                Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52e3c8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.exact_match for e in eval_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69b05fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQLEvaluation(exec_accuracy=True, exec_accuracy_justification=\"The candidate SQL and the golden SQL both aggregate total sales (SUM of sales_tg) per store and join the same tables on store_id. The only difference is an added 'ORDER BY total_revenue DESC' in the candidate, which only changes the row order and does not alter the actual results (store names and their corresponding total revenues). Thus, the output data is the same for both queries, meeting the user's request.\", exact_match=False, exact_match_justification=\"The candidate SQL includes 'ORDER BY total_revenue DESC', which is not present in the golden SQL. While this doesn't affect the query result content, it is a logical difference in how results are presented, violating strict exact match per criteria.\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a42d467",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m [e.exec_accuracy \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[43meval_results\u001b[49m]\n",
      "\u001b[31mNameError\u001b[39m: name 'eval_results' is not defined"
     ]
    }
   ],
   "source": [
    "[e.exec_accuracy for e in eval_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4888209d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQLEvaluation(exec_accuracy=False, exec_accuracy_justification='The candidate SQL counts the number of distinct store_id values in the fact_sales table, which may not match the total number of stores listed in dict_store. The golden SQL counts all entries in dict_store, which is the authoritative list of stores. If fact_sales is missing stores (e.g., stores without recorded sales), the count will differ.', exact_match=False, exact_match_justification=\"The candidate SQL uses a different table (fact_sales) and counts DISTINCT store_id, whereas the golden SQL uses dict_store and counts all rows. The tables and aggregation methods differ, so it's not a logical match.\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e85dc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app.tools as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "001a6c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - TOOL CALL: list_tables()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dict_store', 'dict_sku', 'fact_sales', 'metadata', 'fact_bal', 'fact_cost']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.list_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
